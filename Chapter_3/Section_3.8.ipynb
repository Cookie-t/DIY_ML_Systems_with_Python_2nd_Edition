{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michael/anaconda/lib/python2.7/site-packages/sklearn/preprocessing/data.py:323: DataConversionWarning: Data with input dtype int64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "train = pd.read_csv('datasets/fashionmnist/fashion-mnist_train.csv')\n",
    "\n",
    "\n",
    "y_train = train['label'].values\n",
    "\n",
    "X_train = train.drop('label', axis=1)\n",
    "\n",
    "mm_scaler = MinMaxScaler(feature_range=(0,1))\n",
    "X_train = mm_scaler.fit_transform(X_train)\n",
    "real_samples, dim = X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michael/anaconda/lib/python2.7/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "BATCH_SIZE = 256\n",
    "N_BATCHES = real_samples / BATCH_SIZE\n",
    "N_EPOCHS = 300\n",
    "LEARNING_RATE = 1e-4\n",
    "REAL_INPUT_UNITS = 784\n",
    "HIDDEN_UNITS = 256\n",
    "NOISE_INPUT_UNITS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generator(noise_img, hidden_units, output_dim, reuse=False):\n",
    "    with tf.variable_scope('generator', reuse=reuse):\n",
    "        hidden_layer = tf.layers.dense(noise_img, hidden_units, activation=tf.nn.relu)\n",
    "        outputs = tf.layers.dense(hidden_layer, output_dim, activation=tf.nn.sigmoid)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discriminator(img, hidden_units, reuse=False):\n",
    "    with tf.variable_scope('discriminator', reuse=reuse):\n",
    "        hidden_layer = tf.layers.dense(img, hidden_units, activation=tf.nn.relu)\n",
    "        logits = tf.layers.dense(hidden_layer, 1, activation=None)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: d_loss = 0.111436, g_loss = 3.920505\n",
      "Iteration 2: d_loss = 0.159339, g_loss = 3.184043\n",
      "Iteration 3: d_loss = 0.185887, g_loss = 2.984268\n",
      "Iteration 4: d_loss = 0.257341, g_loss = 2.830972\n",
      "Iteration 5: d_loss = 0.263915, g_loss = 2.237961\n",
      "Iteration 8: d_loss = 0.285273, g_loss = 2.201838\n",
      "Iteration 10: d_loss = 0.334021, g_loss = 2.088378\n",
      "Iteration 11: d_loss = 0.438094, g_loss = 1.885623\n",
      "Iteration 12: d_loss = 0.479500, g_loss = 1.778850\n",
      "Iteration 44: d_loss = 0.630171, g_loss = 1.619900\n",
      "Iteration 156: d_loss = 0.867491, g_loss = 1.550082\n",
      "Iteration 178: d_loss = 1.036903, g_loss = 1.436472\n"
     ]
    }
   ],
   "source": [
    "real_img = tf.placeholder(tf.float32, [None, REAL_INPUT_UNITS], name='real_img')\n",
    "noise_img = tf.placeholder(tf.float32, [None, NOISE_INPUT_UNITS], name='noise_img')\n",
    "\n",
    "gen_outputs = generator(noise_img, HIDDEN_UNITS, REAL_INPUT_UNITS)\n",
    "\n",
    "dis_real_logits = discriminator(real_img, HIDDEN_UNITS)\n",
    "dis_fake_logits = discriminator(gen_outputs, HIDDEN_UNITS, reuse=True)\n",
    "\n",
    "dis_real_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=dis_real_logits, labels=tf.ones_like(dis_real_logits)))\n",
    "dis_fake_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=dis_fake_logits, labels=tf.zeros_like(dis_fake_logits)))                           \n",
    "dis_loss = dis_real_loss + dis_fake_loss\n",
    "\n",
    "gen_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=dis_fake_logits, labels=tf.ones_like(dis_fake_logits)))\n",
    "\n",
    "train_vars = tf.trainable_variables()\n",
    "\n",
    "dis_vars = [var for var in train_vars if var.name.startswith('discriminator')]\n",
    "dis_train_opt = tf.train.AdamOptimizer(LEARNING_RATE).minimize(dis_loss, var_list=dis_vars)\n",
    "\n",
    "gen_vars = [var for var in train_vars if var.name.startswith('generator')]\n",
    "gen_train_opt = tf.train.AdamOptimizer(LEARNING_RATE).minimize(gen_loss, var_list=gen_vars)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "samples = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    \n",
    "    max_dis_loss = -np.inf\n",
    "    min_gen_loss = np.inf\n",
    "    \n",
    "    \n",
    "    for iteration in range(N_EPOCHS):\n",
    "        for i in range(N_BATCHES):\n",
    "            np.random.seed(iteration * N_BATCHES + i)\n",
    "            \n",
    "            indices = np.random.randint(real_samples, size=BATCH_SIZE)\n",
    "            \n",
    "            batch_real_imgs = X_train[indices]\n",
    "            batch_noise_imgs = np.random.uniform(-1, 1, size=(BATCH_SIZE, NOISE_INPUT_UNITS))\n",
    "            \n",
    "            sess.run(dis_train_opt, feed_dict={real_img: batch_real_imgs, noise_img: batch_noise_imgs})\n",
    "            sess.run(gen_train_opt, feed_dict={noise_img: batch_noise_imgs})\n",
    "            \n",
    "        d_loss, g_loss = sess.run([dis_loss, gen_loss], feed_dict={real_img:batch_real_imgs, noise_img: batch_noise_imgs})\n",
    "        \n",
    "        sample_noise = np.random.uniform(-1, 1, size=(10, NOISE_INPUT_UNITS))\n",
    "        \n",
    "        gen_samples = sess.run(generator(noise_img, HIDDEN_UNITS, REAL_INPUT_UNITS, reuse=True),\n",
    "                               feed_dict={noise_img: sample_noise})\n",
    "        samples.append(gen_samples)\n",
    "        \n",
    "        # 存储checkpoints\n",
    "        \n",
    "        if max_dis_loss < d_loss and min_gen_loss > g_loss:\n",
    "            max_dis_loss = d_loss\n",
    "            min_gen_loss = g_loss\n",
    "            saver.save(sess, 'models/section_3.8/my_final_model.ckpt')\n",
    "        \n",
    "            print 'Iteration %d: d_loss = %f, g_loss = %f' % (iteration + 1, max_dis_loss, min_gen_loss)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# 指定要查看的轮次\n",
    "epoch_idx = [0, 5, 10, 20, 50, 100, 150, 250, 290] # 一共300轮，不要越界\n",
    "show_imgs = []\n",
    "for i in epoch_idx:\n",
    "    show_imgs.append(samples[i])\n",
    "\n",
    "rows, cols = len(epoch_idx), 10\n",
    "fig, axes = plt.subplots(figsize=(30,12), nrows=rows, ncols=cols, sharex=True, sharey=True)\n",
    "\n",
    "idx = range(0, N_EPOCHS, int(N_EPOCHS/rows))\n",
    "\n",
    "for sample, ax_row in zip(show_imgs, axes):\n",
    "    for img, ax in zip(sample[::int(len(sample)/cols)], ax_row):\n",
    "        ax.imshow(img.reshape((28,28)), cmap='Greys_r')\n",
    "        ax.xaxis.set_visible(False)\n",
    "        ax.yaxis.set_visible(False)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
